{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9b8bf5-7b6a-4862-8892-6acaedf6d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index in predictor: 3580\n",
      "Total words: 3581\n",
      "Train Data Shape: (6445, 17)\n",
      "Test Data Shape: (1612, 17)\n",
      "Epoch [1/30], Loss: 7.7735\n",
      "Model checkpoint saved at checkpoints/model_epoch_1.pth\n",
      "Epoch [2/30], Loss: 7.2890\n",
      "Model checkpoint saved at checkpoints/model_epoch_2.pth\n",
      "Epoch [3/30], Loss: 7.0338\n",
      "Model checkpoint saved at checkpoints/model_epoch_3.pth\n",
      "Epoch [4/30], Loss: 6.7729\n",
      "Model checkpoint saved at checkpoints/model_epoch_4.pth\n",
      "Epoch [5/30], Loss: 6.4988\n",
      "Model checkpoint saved at checkpoints/model_epoch_5.pth\n",
      "Epoch [6/30], Loss: 6.2122\n",
      "Model checkpoint saved at checkpoints/model_epoch_6.pth\n",
      "Epoch [7/30], Loss: 5.9568\n",
      "Model checkpoint saved at checkpoints/model_epoch_7.pth\n",
      "Epoch [8/30], Loss: 5.6653\n",
      "Model checkpoint saved at checkpoints/model_epoch_8.pth\n",
      "Epoch [9/30], Loss: 5.4405\n",
      "Model checkpoint saved at checkpoints/model_epoch_9.pth\n",
      "Epoch [10/30], Loss: 5.1733\n",
      "Model checkpoint saved at checkpoints/model_epoch_10.pth\n",
      "Epoch [11/30], Loss: 4.9689\n",
      "Model checkpoint saved at checkpoints/model_epoch_11.pth\n",
      "Epoch [12/30], Loss: 4.7600\n",
      "Model checkpoint saved at checkpoints/model_epoch_12.pth\n",
      "Epoch [13/30], Loss: 4.5644\n",
      "Model checkpoint saved at checkpoints/model_epoch_13.pth\n",
      "Epoch [14/30], Loss: 4.3698\n",
      "Model checkpoint saved at checkpoints/model_epoch_14.pth\n",
      "Epoch [15/30], Loss: 4.2263\n",
      "Model checkpoint saved at checkpoints/model_epoch_15.pth\n",
      "Epoch [16/30], Loss: 4.0824\n",
      "Model checkpoint saved at checkpoints/model_epoch_16.pth\n",
      "Epoch [17/30], Loss: 3.9419\n",
      "Model checkpoint saved at checkpoints/model_epoch_17.pth\n",
      "Epoch [18/30], Loss: 3.7903\n",
      "Model checkpoint saved at checkpoints/model_epoch_18.pth\n",
      "Epoch [19/30], Loss: 3.6977\n",
      "Model checkpoint saved at checkpoints/model_epoch_19.pth\n",
      "Epoch [20/30], Loss: 3.5602\n",
      "Model checkpoint saved at checkpoints/model_epoch_20.pth\n",
      "Epoch [21/30], Loss: 3.4850\n",
      "Model checkpoint saved at checkpoints/model_epoch_21.pth\n",
      "Epoch [22/30], Loss: 3.3729\n",
      "Model checkpoint saved at checkpoints/model_epoch_22.pth\n",
      "Epoch [23/30], Loss: 3.3038\n",
      "Model checkpoint saved at checkpoints/model_epoch_23.pth\n",
      "Epoch [24/30], Loss: 3.2340\n",
      "Model checkpoint saved at checkpoints/model_epoch_24.pth\n",
      "Epoch [25/30], Loss: 3.1679\n",
      "Model checkpoint saved at checkpoints/model_epoch_25.pth\n",
      "Epoch [26/30], Loss: 3.0804\n",
      "Model checkpoint saved at checkpoints/model_epoch_26.pth\n",
      "Epoch [27/30], Loss: 3.0204\n",
      "Model checkpoint saved at checkpoints/model_epoch_27.pth\n",
      "Epoch [28/30], Loss: 2.9868\n",
      "Model checkpoint saved at checkpoints/model_epoch_28.pth\n",
      "Epoch [29/30], Loss: 2.9356\n",
      "Model checkpoint saved at checkpoints/model_epoch_29.pth\n",
      "Epoch [30/30], Loss: 2.9164\n",
      "Model checkpoint saved at checkpoints/model_epoch_30.pth\n",
      "Test Accuracy: 5.46%\n",
      "Donald Trump Trade War Energy Landscape It Looks\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "curr_dir = \"/home/hyarrava/Text_generation_using_lstm/\"\n",
    "\n",
    "all_headlines = []\n",
    "\n",
    "for filename in os.listdir(os.path.join(curr_dir, \"data/\")):\n",
    "    if 'Articles' in filename:\n",
    "        articles_df = pd.read_csv(os.path.join(curr_dir, \"data/\", filename))\n",
    "        all_headlines.extend(list(articles_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h!=\"Unknown\"]\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = ''.join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf-8\").decode(\"ascii\", \"ignore\")\n",
    "    return txt\n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "\n",
    "def build_word_index(corpus):\n",
    "    word_index = defaultdict(int)\n",
    "    index =0\n",
    "\n",
    "    for line in corpus:\n",
    "        tokens = word_tokenize(line.lower())\n",
    "        for token in tokens:\n",
    "            if token not in word_index:\n",
    "                word_index[token] = index\n",
    "                index+=1\n",
    "    return word_index\n",
    "\n",
    "\n",
    "def get_sequence_of_tokens(corpus, word_index):\n",
    "    input_sequences = [] \n",
    "    for line in corpus:\n",
    "        tokens = word_tokenize(line.lower())\n",
    "        token_list = [word_index[token] for token in tokens] ## list of tokens for each sentence or line       \n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "            \n",
    "    return input_sequences\n",
    "\n",
    "word_index = build_word_index(corpus)\n",
    "total_words = len(word_index)\n",
    "input_sequences = get_sequence_of_tokens(corpus, word_index)\n",
    "\n",
    "### Padding\n",
    "\n",
    "def to_categorical_numpy(labels, num_classes):\n",
    "    # Create a zero matrix of shape (number of labels, number of classes)\n",
    "    one_hot_encoded = np.zeros((labels.size, num_classes))\n",
    "    \n",
    "    # Set the appropriate elements to 1 based on the labels\n",
    "    one_hot_encoded[np.arange(labels.size), labels] = 1\n",
    "    \n",
    "    return one_hot_encoded\n",
    "\n",
    "def making_features_labels(sequences):\n",
    "    predictor, labels = sequences[:,:-1], sequences[:,-1]\n",
    "    labels = to_categorical_numpy(labels, total_words)  # total_words should be defined globally or passed in\n",
    "    return predictor, labels\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, padding='pre', truncating='pre', value=0):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) < maxlen:\n",
    "            if padding == 'post':\n",
    "                padded_seq = seq + [value] * (maxlen - len(seq))\n",
    "            else:\n",
    "                padded_seq = [value] * (maxlen - len(seq)) + seq\n",
    "        else:\n",
    "            padded_seq = seq\n",
    "\n",
    "        if len(seq) > maxlen:\n",
    "            if truncating == 'pre':\n",
    "                truncated_seq = seq[-maxlen:]\n",
    "            else:\n",
    "                truncated_seq = seq[:maxlen]\n",
    "\n",
    "        padded_sequences.append(padded_seq)\n",
    "\n",
    "    return np.array(padded_sequences), maxlen\n",
    "\n",
    "\n",
    "# Revised workflow to generate features and labels\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    padded_sequences, max_sequence_len = pad_sequences(input_sequences, padding='pre')\n",
    "    predictors, labels = making_features_labels(padded_sequences)\n",
    "    \n",
    "    return predictors, labels, max_sequence_len\n",
    "\n",
    "\n",
    "predictor, labels, max_sequence_len = generate_padded_sequences(input_sequences)\n",
    "\n",
    "\n",
    "predictor.shape, labels.shape\n",
    "\n",
    "max_index = np.max(predictor)\n",
    "print(\"Max index in predictor:\", max_index)\n",
    "print(\"Total words:\", total_words)\n",
    "\n",
    "# Ensure that max_index is within bounds\n",
    "if max_index >= total_words:\n",
    "    raise ValueError(f\"Index {max_index} in predictor exceeds the maximum index allowed ({total_words - 1}). Please check your data preprocessing.\")\n",
    "\n",
    "# Check for negative indices\n",
    "if np.any(predictor < 0):\n",
    "    raise ValueError(\"Negative indices found in predictor. All indices should be non-negative.\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictor, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train Data Shape:\", X_train.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)\n",
    "\n",
    "## Model creation\n",
    "\n",
    "class Text_generation(nn.Module):\n",
    "    def __init__(self, maxlen_seq, total_words):\n",
    "        super().__init__()\n",
    "        self.num_lstm_layers = 3\n",
    "        self.layer1 = nn.Embedding(num_embeddings=total_words, embedding_dim=256)\n",
    "        self.layer2 = nn.LSTM(256, 32, num_layers = self.num_lstm_layers,  batch_first=True, dropout=0.3, bidirectional = True)\n",
    "        self.layer3 = nn.Dropout(0.3)\n",
    "        self.layer4 = nn.Linear(32*2, total_words) ## *2 is for bidirectional\n",
    "        self.layernorm = nn.LayerNorm(32*2)\n",
    "    def forward(self, predictor):\n",
    "        out = self.layer1(predictor)         # Embedding layer\n",
    "        out, (hn, cn) = self.layer2(out)     # LSTM returns (output, (h_n, c_n)), unpack both\n",
    "        out = self.layernorm(out[:, -1, :])\n",
    "        out = self.layer3(out)               # Dropout layer (expects a tensor, not a tuple)\n",
    "        out = self.layer4(out)     # Only take the output of the last time step from LSTM\n",
    "        return out\n",
    "\n",
    "def prepare_dataset(X_train, y_train, X_test, y_test, batch_size=8):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32) \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    \n",
    "    # Create DataLoader for batch processing\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Training function with checkpoint saving\n",
    "def train_model_with_checkpoint(model, train_loader, criterion, optimizer, num_epochs=20, checkpoint_dir='checkpoints/'):\n",
    "    # Create a directory to save checkpoints if it doesn't exist\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "        \n",
    "        # Save the model checkpoint after every epoch\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss / len(train_loader),\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)   # Binary classification threshold\n",
    "            labels = torch.argmax(labels, dim =1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare data (assuming X_train, y_train, X_test, y_test are already prepared as numpy arrays)\n",
    "train_loader, test_loader = prepare_dataset(X_train, y_train, X_test, y_test, batch_size)\n",
    "\n",
    "# Initialize model, criterion (loss function), and optimizer\n",
    "model = Text_generation(max_sequence_len, total_words)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model = train_model_with_checkpoint(model, train_loader, criterion, optimizer, num_epochs)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "def build_index_word(word_index):\n",
    "    return {index: word for word, index in word_index.items()}\n",
    "\n",
    "def encode_text(text, word_index):\n",
    "    tokens = text.lower().split()  # Tokenize and lower the text\n",
    "    return [word_index[token] for token in tokens if token in word_index]\n",
    "\n",
    "def decode_index(index, index_word):\n",
    "    return index_word.get(index, \"\")\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len, word_index, index_word):\n",
    "    for _ in range(next_words):\n",
    "        # Encode the seed text\n",
    "        token_list = encode_text(seed_text, word_index)\n",
    "        \n",
    "        # Pad the sequences\n",
    "        token_list, _ = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        # Convert to tensor and ensure it's of long type\n",
    "        token_tensor = torch.tensor(token_list, dtype=torch.long).squeeze(0)  # Remove any extra dim if it exists\n",
    "\n",
    "        # Add a batch dimension (batch_size=1)\n",
    "        token_tensor = token_tensor.unsqueeze(0)  # Shape: (1, max_sequence_len-1)\n",
    "        \n",
    "        # Predict the next word index\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            predicted = model(token_tensor).argmax(dim=1).item()  # Use argmax to get the predicted index\n",
    "\n",
    "        # Map the predicted index back to a word\n",
    "        output_word = decode_index(predicted, index_word)\n",
    "        \n",
    "        # Append the predicted word to the seed text\n",
    "        seed_text += \" \" + output_word\n",
    "        \n",
    "    return seed_text.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a4caf1-8dbf-4890-8d02-d56d3e9df9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Of Killer’S Killers Widow Scared Victim Of Stormy Danielss Legal Fund Joins North Korea Gamble Or Marvel China Plans To Trump For\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "index_word = build_index_word(word_index)\n",
    "print(generate_text(\"Trial of Killer’s\", 20, model, max_sequence_len, word_index, index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b5102-775c-459c-9964-a6340d09ace4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
